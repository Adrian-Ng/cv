%-------------------------
% Resume in Latex
% Author : Sourabh Bajaj
% License : MIT
%------------------------

\documentclass[letterpaper,11pt]{article}
\usepackage{latexsym}
\usepackage[empty]{fullpage}
\usepackage{titlesec}
\usepackage{marvosym}
\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage[inline]{enumitem}
% change pdftex to tex4ht when outputting odt file!
\usepackage[pdftex]{hyperref}
\usepackage{fancyhdr}
\usepackage[symbol]{footmisc}
\usepackage{multicol}


\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Adjust margins
\addtolength{\oddsidemargin}{-0.375in}
\addtolength{\evensidemargin}{-0.375in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1.0in}
% https://tex.stackexchange.com/questions/53962/why-are-urls-typeset-with-monospace-fonts-by-default
\urlstyle{same}

%%\raggedbottom
%\raggedright
\setlength{\tabcolsep}{0in}
\linespread{0.97}


% Sections formatting
\titleformat{\section}{
	\vspace{-4pt}\scshape\raggedright\large
}{}{0em}{}[\color{black}\titlerule \vspace{-5pt}]

%-------------------------
% Custom commands
\newcommand{\resumeItem}[3]{
	\item\small{
		\textbf{#1}\hfill\tiny{#2\\}\small{ #3 \vspace{-2pt}}
	}
}

\newcommand{\resumeSubheading}[4]{
	\vspace{-1pt}\item
	\begin{tabular*}{0.97\textwidth}{l@{\extracolsep{\fill}}r}
		\textbf{#1} & #2 \\
		\textit{\small#3} & \textit{\small #4} \\
	\end{tabular*}\vspace{-5pt}
}

\newcommand{\resumeSubItem}[3]{\resumeItem{#1}{#2}{#3}\vspace{-2pt}}

\renewcommand{\labelitemii}{$\circ$}

\newcommand{\resumeSubHeadingListStart}{\begin{itemize}[leftmargin=*]}
	\newcommand{\resumeSubHeadingListEnd}{\end{itemize}}
\newcommand{\resumeItemListStart}{\begin{itemize}}
	\newcommand{\resumeItemListEnd}{\end{itemize}\vspace{-5pt}}
	\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%-------------------------------------------
%%%%%%  CV STARTS HERE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%----------HEADING-----------------
\noindent
\Large \textbf{Adrian Ng, MSc.}
\newline
\small
Seeking Junior-Level Data Engineering Opportunities
\hfill
\begin{description*}
	\item [Email:] \href{mailto:contact@adrian.ng}{contact@adrian.ng}
	\item [Website:] \href{https://adrian.ng}{adrian.ng}
\end{description*}
%----------PROFILE-----------------
\section{Profile}
\begin{paragraph}
	I am a Computer Science graduate passionate about programming and a career in Data Engineering. I seek opportunities that meet my growing experience in \textit{Java} -- a language I have used in numerous academic projects ranging from the implementation of financial models to large-scale data processing with \textit{Apache Hadoop} and more.
	%In addition, \texttt{R} and \texttt{MATLAB} were used to implement various machine learning algorithms.
	\noindent
	\newline
	Prior to postgraduate study, my expertise in \textit{SQL development} focused on the implementation of segmentation processes for a number of clients including: \textit{Virgin Media}, \textit{TUI}, \textit{UPC}, \textit{MSD}, \textit{Volkswagen}, and \textit{KwikFit}.
	\noindent
	\newline
	After graduation, my accomplishments as a Data Analyst at \textit{Manchester City FC} leaned more towards Data Engineering, which leads me now to pursue a career in this field.
\end{paragraph}
%-----------EDUCATION-----------------
\section{Education}
\resumeSubHeadingListStart

\resumeSubheading
{Royal Holloway -- Department of Computer Science}{Sept. 2016 -- Dec. 2017}
{Master of Science in Data Science and Analytics}{with Distinction}

\resumeSubheading
{King's College London -- School of Engineering}{Sept. 2007 -- July 2010}
{Bachelor of Engineering in Mechanical Engineering}{Upper Second Class with Honours}

\resumeSubHeadingListEnd

%--------PROGRAMMING SKILLS------------
\section{Technologies}
\textbf{Languages:}
\hfill
\begin{itemize*}
	\item Java 8
	\item SQL
\end{itemize*}
\newline
\textbf{Software:}
\hfill
\begin{itemize*}
	\item IntelliJ IDEA
	\item SQL Server Management Studio
	\item Git
	\item VS Code
	\item Jira
	\item Maven
\end{itemize*}

%-----------PROJECTS-----------------
\section{Java Projects}
%\subsection{VaR}
\textbf{Implementation of Value at Risk (VaR) measure}
\hfill
\tiny
(\href{https://adrian.ng/java/var/}{https://adrian.ng/java/var/})
\hfill
(\href{https://github.com/Adrian-Ng/VaR}{https://github.com/Adrian-Ng/VaR})
\newline
\small
I implemented a number of approaches to estimating \textit{VaR}, a measure of risk, for a hypothetical investment portfolio containing stocks, options, and corresponding deltas. I utilised \textit{Google Fiance}/\textit{Yahoo Finance} APIs to read time-series data, for which I implemented a number of \textit{moving average processes} for estimating variance.
%https://tex.stackexchange.com/questions/233780/two-itemized-lists-side-by-side
\begin{multicols}{2}
	\textbf{VaR Measures}
	\begin{itemize}
		\item Model Building
		\item Historical Simulation
		\item Monte Carlo Simulation.
	\end{itemize}
	\columnbreak
	\textbf{Moving Average Processes}
	\begin{itemize}
		\item \textit{Equal Weighted}
		\item \textit{Exponentially Weighted Moving Average (EWMA)}
		\item \textit{GARCH(1,1)}
	\end{itemize}
\end{multicols}
\noindent
In addition, an implementation of the \textit{Levenberg-Marquardt} algorithm was used for optimisation of \textit{GARCH(1,1)} parameters via maximum likelihood estimation.
I made use of object-oriented techniques and patterns to accommodate these numerous approaches.
I used \texttt{Java's} concurrency APIs to parallelize the 100,000+ random walks generated by \textit{Monte Carlo} when simulating stock price movements, which resulted in a highly efficient solution.
\newline \newline
\noindent
\textbf{Large-Scale data processing with Apache Hadoop}
\hfill
\tiny
(\href{https://github.com/Adrian-Ng/HadoopEnron}{https://github.com/Adrian-Ng/HadoopEnron})
\small
\newline
During my postgraduate module \textit{Large Scale Data Storage and Processing}, I wrote a number of \textit{MapReduce} applications. These projects included:
\begin{itemize*}
	\item the aggregation of \textit{Twitter} data
	\item scraping a large collection of emails in the \textit{Enron Corpus}
	\item the extraction of nodes/edges from this communications network.
\end{itemize*}
I ran my applications on a self-hosted, single-node cluster as well as on the university's distributed cluster. To load/extract data in \textit{HDFS}, I used \texttt{hdfs dfs} commands.
\begin{itemize}
	\item
	      \textbf{Apache Spark}
	      \hfill
	      \tiny
	      (\href{https://adrian.ng/scala/spark/enron1}{https://adrian.ng/scala/spark/enron1})
	      \small
	      \newline
	      In a self-learning exercise, I translated some of these \textit{MapReduce} applications to \texttt{Scala}. This code was less verbose and ran in an \textit{Apache Spark REPL}, which could still interface with \textit{HDFS} via \texttt{sparkcontext} APIs.
\end{itemize}
\noindent
\textbf{Option Pricing}
\hfill
\tiny
(\href{https://adrian.ng/java/options/}{https://adrian.ng/java/options/}) \quad (\href{https://github.com/Adrian-Ng/OptionPricer}{https://github.com/Adrian-Ng/OptionPricer})
\newline
\small
As part of the postgraduate module, \textit{Methods of Computational Fianance}, I implemented three approaches to estimating option prices:
\begin{itemize*}
	\item Monte Carlo Simulation
	\item Black Scholes
	\item Binomial Trees.
\end{itemize*}
And where applicable, I computed the \textit{payoff} for American, Asian, and European options.
These approaches made probabilistic assumptions, so \textit{Apache Commons Math} API was used.
\newline \newline
\noindent
\textbf{Java 8 Streams with financial data}
\hfill
\tiny
(\href{https://adrian.ng/java/yahoofinance/\#stream}{https://adrian.ng/java/yahoofinance/\#stream})
\small
\newline
A self-taught exercise to gain familiarity with Java 8's \texttt{Stream} API. I was able to implement approaches to computing mean and variance estimators from an immutable collection of time-series financial data.

%		 \resumeSubItem{Google PageRank}{}
%		 {
%		   This is the implementation of Google's \textit{PageRank} algorithm. I simulate the behaviour of someone browsing a series of webpages by computing a transition matrix from an input graph and mixing a Markov Chain.
%		 }   


% %-----------COURSEWORK-----------------
% \section{Machine Learning Algorithms Implemented}
% \textbf{R}
% \hfill
% \begin{itemize*}
% 	\item k-Nearest Neighbours
% 	\item LDA
% 	\item Neural Networks
% 	\item Decision Trees
% 	\item Hierarchical Clustering
% \end{itemize*}
% \newline
% \textbf{MATLAB}
% \hfill
% \begin{itemize*}
% 	\item Hidden Markov Models
% 	\item Aggregating Algorithm
% \end{itemize*}

\newpage
%-----------EXPERIENCE-----------------
\section{Manchester City Football Club}
\textit{Data Analyst}
\hfill
\textit{Fan Relationship Management}
\hfill
\textit{Jan. - July 2018}
\newline \newline
\textbf{New York City FC Project:}
I took ownership of this project to integrate \textit{NYCFC's} transactional and demographic data with \textit{City Football Group's} data-warehouse. This six-month project involved many phases including: discovery, engineering, and analysis. Data came from multiple external sources each with differing schema: \textit{NYCFC}, \textit{Ticketmaster} \textit{Salesforce}, \textit{Major League Soccer}.
\begin{itemize}
	\item
	      {
	      \textbf{Data Pipeline:}
	      I implemented a data pipeline to ingress data from multiple databases.
	      This process was encapsulated in \textit{stored procedures} in which I wrote appropriate DML \& DDL (\texttt{OPENQUERY}, \texttt{MERGE}) for efficient ETL.
	      This pipeline replaced the slower front-end  \textit{Informatica} solution.
	      }
	\item
	      {
	      \textbf{Data Cubes:}
	      I used an aggregated dataset to compare the distribution of \texttt{NULL} values. These analyses were transformed to \textit{Data Cubes} to pre-compute every possible roll-up/drill-down. As such, bandwidth was minimised across our distributed servers and need for real-time computation in \textit{Tableau} front-end was eliminated, resulting in an improved user-experience.
	      }
	\item
	      {
	      \textbf{Mentoring:}
	      As part of this project, I dedicated time to mentoring a junior colleague remotely in New York. I organised weekly workshops to teach basic DML and more advanced DDL with a goal toward self-sufficiency in writing database queries and stored procedure implementation/scheduling. Additional material on my website helped supplement these workshops.
	      }
\end{itemize}
\noindent
\textbf{GDPR Preference Pipeline:}
I worked on the integration of a GDPR preference pipeline into our data stores (\textit{SQL}, \textit{Salesforce}) and the subsequent refactoring of numerous processes downstream.
I worked with SQL developers to provide specification and UAT testing. I built an efficient, automated \texttt{MERGE} process using relational database design (primary key constraints, clustered indexes, triggers).
\newline \newline
\noindent
\textbf{Customer Churn Model:}
In an intra-team project to estimate MCFC/NYCFC customers' propensity to churn in following seasons, I assisted with feature selection thereby contributing to data extraction, imputation, and normalisation. Model selection landed on \textit{logistic regression} but I also researched alternate models (involving more probabilistic assumptions) such as \textit{Beta-Geometric/Beta-Bernoulli}; for which I looked at the corresponding academic papers and \texttt{R Studio} API.

\section{Creator (now Inspired Thinking Group)}
\textit{Senior CRM Campaign Executive}
\hfill
\textit{SQL Development}
\hfill
\textit{Dec. 2013 - Sept. 2016\\}

\noindent
The majority of my work in this role involved working with \texttt{SQL} processes which were used to transform customer data into CRM segmentations. Having been promoted to the senior position, I developed a number of these processes. On occasion, I took responsibility for resourcing and managing the team's workload using \textit{Jira}.
\newline \newline
\noindent
\textbf{Virgin Media Segmentation}
\hfill
\tiny
(\href{https://adrian.ng/SQL/cte/Recursion/}{https://adrian.ng/SQL/cte/Recursion/}
\quad
(\href{https://adrian.ng/SQL/misc/openquery-xml}{https://adrian.ng/SQL/misc/openquery-xml})
\newline
\small
I built an end-to-end segmentation process in \texttt{SQL}. This included building a fast, flexible, and bespoke import tool around \texttt{BULK INSERT} to efficiently ingest and union millions of tuples distributed across multiple flat-files. Remote server queries (\texttt{OPENQUERY}) made use of \texttt{XML} to effectively \texttt{INNER JOIN} local and remote tables resulting in speed and minimial resource use on a busy live server. I used recursive queries to implement efficient regex parsing similar to \texttt{flatMap} in \texttt{Java 8}/\texttt{Scala}.
\newline \newline
\noindent
\textbf{Volkswagen Onboarding:}
I worked with \texttt{.NET} developers and project managers to bring Volkswagen on-board as a new client. This required building and testing a new segmentation process for broadcasting email \textit{and} SMS. In addition, I provided specification to developers for their data warehousing/archiving ingress schema.
\newline \newline
\noindent
\textbf{TUI Redesign:}
I collaborated with the TUI client to integrate a new design of their large deployment broadcasts (5M+ recipients) for \textit{Thomson} and \textit{First Choice} brands. \texttt{TCL} scripts were developed to dynamically merge fields into the \texttt{HTML} body and, where possible, efficienices were gained by moving expensive operations upstream. Over the course of this three-month project, I gained recognition with the client and was awarded for my efforts. \textcolor{red}{-- they sent me an e-card! how best to say this??}

\section{Seatwave (now Ticketmaster)}
\textit{Marketing Analyst Intern}
\hfill
\textit{Commercial Team}
\hfill
\textit{May 2013 - Dec. 2013\\}

\noindent
Using \textit{SQL Server Management Studio}, I wrote DML capable of querying the transactional/customer databases to return data for warehousing, reporting, and segmentation. I also worked on pricing and spatial analyses, using \textit{QGIS} as a visualisation tool.
\end{document}